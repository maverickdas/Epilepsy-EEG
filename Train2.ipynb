{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as osp\n",
    "from datetime import datetime\n",
    "import ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import keras.backend as K\n",
    "import keras as keras\n",
    "from keras.models import Sequential,Model\n",
    "# from keras.metrics import mae, categorical_accuracy\n",
    "\n",
    "from keras.layers import MaxPooling1D, Dense, Dropout, Flatten, Input, Conv1D, LeakyReLU, BatchNormalization, Softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # curr_path = os.getcwd()\n",
    "# # curr_path\n",
    "# # lossfn = LeakyReLU(alpha=0.02)\n",
    "# optim = 'adam'\n",
    "# lossfn = 'relu'\n",
    "# BATCH = False\n",
    "# EPOCHS = 500\n",
    "# DROPRATE = 0.4\n",
    "# classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_dict = {}\n",
    "if classes == 4:\n",
    "    fold_dict = {\"O\":[0, 0,0,1],\"F\":[0, 0,1,0], 'S':[0, 1,0,0], \"N\":[1, 0,0,0]}\n",
    "elif classes == 3:\n",
    "    fold_dict = {\"O\":[0,0,1],\"F\":[0,1,0], 'S':[1,0,0]}\n",
    "# out_len = len(fold_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ID = \"cls:{}_loss:{}_bnm:{}_drop:{}_epo:{}_opt:{}\".format(classes, lossfn, BATCH, DROPRATE, EPOCHS, optim)\n",
    "PLT_TITLE = \"{} classes, {} actvn, {} BN, {} dropout, {} opt\".format(classes, lossfn, BATCH, DROPRATE, optim)\n",
    "TEST_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_time = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "data_dir = 'data'\n",
    "chk_dir = 'checkpoints'\n",
    "plot_dir = osp.join('plots', EXP_ID)\n",
    "logdir = \"logs/scalars/\" + EXP_ID +\"/\" + TEST_ID\n",
    "logdir = osp.join(logdir, curr_time)\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "for dirn in [data_dir, chk_dir, plot_dir]:\n",
    "    os.makedirs(dirn, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(fold,label):\n",
    "    fold_arr = []\n",
    "    labels_arr = []\n",
    "    files = []\n",
    "    for f in os.listdir(fold):\n",
    "        files.append(osp.join(fold, f))\n",
    "\n",
    "\n",
    "    for f in files:\n",
    "\n",
    "        lines = []\n",
    "        with open(f, 'r') as fw:\n",
    "            for i, line in enumerate(fw):\n",
    "                lines.append(int(line.split()[0]))\n",
    "        lines_arr = np.array(lines)\n",
    "        lines_arr=(lines_arr-np.mean(lines_arr))/np.var(lines_arr)\n",
    "        fold_arr.append(lines_arr)\n",
    "        labels_arr.append(label)\n",
    "    return fold_arr,labels_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_list = []\n",
    "train_X = []\n",
    "train_Y = []\n",
    "for key,val in fold_dict.items():\n",
    "    x,y = extract(osp.join(data_dir,key),val)\n",
    "    train_X.extend(x)\n",
    "    train_Y.extend(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_X),len(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_lay = Input((4097,1))\n",
    "\n",
    "\n",
    "l1 = Conv1D(4, kernel_size = 6, strides=1, padding = 'same',activation = lossfn)(in_lay)\n",
    "ml1 = MaxPooling1D(pool_size=2, strides=2)(l1)\n",
    "if BATCH: ml1 = BatchNormalization()(ml1)\n",
    "\n",
    "l1 = Conv1D(4, kernel_size = 5, strides=1, padding = 'same',activation = lossfn)(ml1)\n",
    "ml1 = MaxPooling1D(pool_size=2, strides=2)(l1)\n",
    "if BATCH: ml1 = BatchNormalization()(ml1)\n",
    "\n",
    "l1 = Conv1D(10, kernel_size = 4, strides=1, padding = 'same',activation = lossfn)(ml1)\n",
    "ml1 = MaxPooling1D(pool_size=2, strides=2)(l1)\n",
    "if BATCH: ml1 = BatchNormalization()(ml1)\n",
    "\n",
    "l1 = Conv1D(10, kernel_size = 4, strides=1, padding = 'same',activation = lossfn)(ml1)\n",
    "ml1 = MaxPooling1D(pool_size=2, strides=2)(l1)\n",
    "if BATCH: ml1 = BatchNormalization()(ml1)\n",
    "\n",
    "l1 = Conv1D(15, kernel_size = 4, strides=1, padding = 'same',activation = lossfn)(ml1)\n",
    "ml1 = MaxPooling1D(pool_size=2, strides=2)(l1)\n",
    "if BATCH: ml1 = BatchNormalization()(ml1)\n",
    "\n",
    "l1 = Conv1D(15, kernel_size = 4, strides=1, padding = 'same',activation = lossfn)(ml1)\n",
    "ml1 = MaxPooling1D(pool_size=2, strides=2)(l1)\n",
    "if BATCH: ml1 = BatchNormalization()(ml1)\n",
    "    \n",
    "# l1 = Conv1D(20, kernel_size = 3, strides=1, padding = 'same',activation = lossfn)(ml1)\n",
    "# ml1 = MaxPooling1D(pool_size=2, strides=2)(l1)\n",
    "# if BATCH: ml1 = BatchNormalization()(ml1)\n",
    "\n",
    "flat = Flatten()(ml1)\n",
    "flat = Dropout(DROPRATE)(flat)\n",
    "\n",
    "flat = Dense(50)(flat)\n",
    "flat = Dropout(DROPRATE)(flat)\n",
    "\n",
    "flat = Dense(20)(flat)\n",
    "flat = Dropout(DROPRATE)(flat)\n",
    "\n",
    "flat = Dense(classes)(flat)\n",
    "\n",
    "flat = Softmax()(flat)\n",
    "\n",
    "model=Model(inputs = [in_lay], outputs = [flat])\n",
    "model.compile(optimizer = optim, \n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "#                                                                              , mae, categorical_accuracy])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path = osp.join(chk_dir, \n",
    "                       \"{}_{}_weights.best.hdf5\".format('epilepsy', \n",
    "                                                        TEST_ID))\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                   factor=0.998, \n",
    "                                   patience=10, \n",
    "                                   verbose=1, \n",
    "                                   mode='auto', \n",
    "                                   min_delta=0.0001, \n",
    "                                   cooldown=5, \n",
    "                                   min_lr=0.0000001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=150)\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat, tb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = np.array(train_X)\n",
    "LABEL = np.array(train_Y)\n",
    "\n",
    "DATA = DATA.reshape((DATA.shape[0], DATA.shape[1], 1))\n",
    "# LABEL = LABEL.reshape((LABEL.shape[0], LABEL.shape[1], 1))\n",
    "\n",
    "# DATA.shape, LABEL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "hist=model.fit(DATA,LABEL, \n",
    "               batch_size=900,\n",
    "               validation_split=0.1,\n",
    "               callbacks = callbacks_list,\n",
    "               epochs=EPOCHS, \n",
    "               shuffle=True,\n",
    "               verbose=0\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\n",
    "fig.suptitle(PLT_TITLE)\n",
    "plt.figure(figsize=(40, 4))\n",
    "ax1.plot(hist.history['accuracy'])\n",
    "ax1.plot(hist.history['val_accuracy'])\n",
    "# ax1.set_title('Accuracy\\n {}'.format(PLT_TITLE))\n",
    "ax1.set_ylabel('accuracy')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.legend(['train', 'test'], loc='lower right')\n",
    "\n",
    "ax2.plot(hist.history['loss'])\n",
    "ax2.plot(hist.history['val_loss'])\n",
    "# ax2.set_title('Loss\\n {}'.format(PLT_TITLE))\n",
    "ax2.set_ylabel('loss')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "fig.savefig('{}/acc_loss_{}.png'.format(plot_dir, TEST_ID))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(hist.history['accuracy']), max(hist.history['val_accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(hist.history['loss']), min(hist.history['val_loss']) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
